{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import json\n",
    "import argparse\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from plot_utils import extract_measures, extract_utterance, get_distributions, get_x_positions, order_lists, plot_reward, plot_training_curve \n",
    "\n",
    "sns.set()\n",
    "# plt.set\n",
    "# plt.figure.\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "dir_ = 'final/new/'\n",
    "\n",
    "filenames = {\n",
    "    'self_both': dir_ + 'log_20191116_215417final_self_2.log',\n",
    "    'self_linguistic': dir_ +'log_20191105_181425final_self_disable-proposal.log',\n",
    "    'self_proposal': dir_ + 'log_20191106_174032final_self_disable-comms.log',\n",
    "    'self_none': dir_ + 'log_20191109_114924final_self_disable-proposal_disable-comms.log',\n",
    "    \n",
    "    'prosoc_both': dir_ + 'log_20191103_201228final_prosoc.log',\n",
    "    'prosoc_linguistic': dir_ + 'log_20191104_193106final_prosoc_disable-proposal.log',\n",
    "    'prosoc_proposal': dir_ + 'log_20191104_145248final_prosoc_disable-comms.log',\n",
    "    'prosoc_none': dir_ + 'log_20191107_200112final_prosoc_disable-proposal_disable-comms.log'  \n",
    "}\n",
    "\n",
    "filenames_memory_comp = {\n",
    "    'prosoc_none': dir_ + 'log_20191108_182251final_prosoc_disable-proposal_disable-comms_memory-comp.log',\n",
    "    'prosoc_linguistic': dir_ + 'log_20191111_090756final_prosocial_memory-comp_disable-proposal.log',\n",
    "    'prosoc_both': dir_ + 'log_20191110_105512final_prosocial_memory-comp.log',\n",
    "    'prosoc_proposal': dir_ + 'log_20191112_174358final_prosocial_memory-comp_disable-comms.log',\n",
    "    \n",
    "    'self_proposal':dir_ + 'log_20191113_185154final_self_memory-comp_disable-comms.log',\n",
    "    'self_linguistic': dir_ + 'log_20191114_095448final_self_memory-comp_disable-proposal.log',\n",
    "    'self_both': dir_ + 'log_20191115_090601final_self_memory-comp.log',\n",
    "    'self_none': dir_ + 'log_20191116_113959final_self_disable-proposal_disable-comms_memory-comp.log'\n",
    "}\n",
    "\n",
    "output_dir = 'figs/'\n",
    "    \n",
    "def double_check(filenames):\n",
    "    for key, filename in filenames.items():\n",
    "        try:\n",
    "            with open(filename, 'r') as f:\n",
    "                    for n, line in enumerate(f):\n",
    "                        if n == 0:\n",
    "                            msg = line\n",
    "                        break\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            msg = 'Somethings wrong in here'\n",
    "        print(key, msg)\n",
    "\n",
    "def gen_iter(exclude_sociality=None, exclude_channels=None):\n",
    "    \"\"\"\n",
    "    exclude_sociality: str\n",
    "    exclude_channels: list of str\n",
    "    \"\"\"\n",
    "    socialities = ['self', 'prosoc']\n",
    "    channels = ['proposal', 'linguistic', 'both', 'none']\n",
    "    if exclude_sociality:\n",
    "        socialities.remove(exclude_sociality)\n",
    "    if exclude_channels:\n",
    "        for channel in exclude_channels:\n",
    "            channels.remove(channel)\n",
    "    return itertools.product(socialities, channels)\n",
    "\n",
    "print('double check filenames:')\n",
    "double_check(filenames)\n",
    "print('\\ndouble check filenames_memory_comp:')\n",
    "double_check(filenames_memory_comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_reward_all(filenames, prefix=''):\n",
    "    for sociality, channel in gen_iter():\n",
    "        key = sociality + '_' + channel\n",
    "        print(key)\n",
    "        filename = filenames[key] if key in filenames else None\n",
    "\n",
    "        if filename:\n",
    "            output = '{}reward_{}_{}.png'.format(output_dir, key, prefix)\n",
    "            plot_reward(filename, 0, 1, key, 200000, labels=None, output_file=output)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reward_all(filenames)\n",
    "plot_reward_all(filenames_memory_comp, 'memory-comp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def joint_reward_success(filenames):\n",
    "\n",
    "    \"\"\"\n",
    "    FOR TABLE 2\n",
    "    \n",
    "    keys:\n",
    "    self_proposal, self_linguistic, self_both, self_none,\n",
    "    prosoc_proposal, prosoc_linguistic, prosoc_both, prosoc_none\n",
    "    \n",
    "    values:\n",
    "    filenames\n",
    "    \n",
    "    Joint reward success and average number of turns taken for paired agents negotiating\n",
    "    with random game termination, varying the agent reward scheme and communication channel.\n",
    "    \"\"\"\n",
    "    \n",
    "    from_paper = {'self_proposal': 0.87, 'self_linguistic': 0.75, 'self_both': 0.87, 'self_none': 0.77,\n",
    "                  'prosoc_proposal': 0.93,  'prosoc_linguistic': 0.99, 'prosoc_both': 0.92, 'prosoc_none': 0.95}\n",
    "    \n",
    "    data = {}\n",
    "    \n",
    "    for sociality, channel in gen_iter():\n",
    "        key = sociality + '_' + channel\n",
    "        filename = filenames.get(key, None)\n",
    "\n",
    "        if filename:\n",
    "            extracted = extract_measures(filenames[key], ['test_reward'])\n",
    "            joint_reward = np.mean(extracted['test_reward'])\n",
    "\n",
    "        else:\n",
    "            joint_reward = -1\n",
    "        data[key] = {'joint_reward': joint_reward}\n",
    "            \n",
    "    for sociality, channel in gen_iter():\n",
    "        key = sociality + '_' + channel\n",
    "        print(sociality + ' ' + channel)\n",
    "        print('\\tour:       {}'.format(data[key]['joint_reward']))\n",
    "        print('\\tfrom paper: {}'.format(from_paper[key]))\n",
    "    return data\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('without memory-comp:\\n')\n",
    "joint_reward_success(filenames)\n",
    "print('\\n\\nmemory-comp:')\n",
    "joint_reward_success(filenames_memory_comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_training_curve_all(filenames, prefix=''):\n",
    "    \"\"\"\n",
    "    FOR FIGURE 2a\n",
    "    \n",
    "    Training curves for SELF-INTERESTED agents learning to negotiate under the various com- munication channels.\n",
    "    \"\"\"\n",
    "\n",
    "    for sociality, channel in gen_iter():\n",
    "        key = sociality + '_' + channel\n",
    "        filename = filenames.get(key, None)\n",
    "        if filename:\n",
    "            output = '{}training_curve_{}{}.png'.format(output_dir, key, prefix)\n",
    "            plot_training_curve(filename, min_y=0, max_y=1, title='', max_x=200000, labels=None, output=output)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_training_curve_all(filenames_memory_comp, '_memory-comp')\n",
    "plot_training_curve_all(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_utterance(distribution, turn, vocab_len=10, utter_len=6, output_dir=False, key=False):\n",
    "    labels = [str(i) for i in range(utter_len)]\n",
    "    \n",
    "    x = np.arange(utter_len)\n",
    "    width = 4\n",
    "    positions = get_x_positions(vocab_len, utter_len, width=width, outer_width=2).T\n",
    "    \n",
    "    plt.clf()\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    plt.ylim((0, 1.05))\n",
    "    reacts = []\n",
    "    \n",
    "    for i in range(vocab_len):\n",
    "        values = distribution[turn, :, i]\n",
    "        l = positions[i]\n",
    "        reacts.append(ax.bar(l, values, width, label=str(i)))\n",
    "    ax.legend()\n",
    "    if output_dir and key:\n",
    "        output = '{}unigrams{}.png'.format(output_dir, key)\n",
    "        plt.savefig(output)\n",
    "        \n",
    "    def autolabel(rects):\n",
    "        \"\"\"\n",
    "        Attach a text label above each bar in *rects*, displaying its height.\n",
    "        from: https://matplotlib.org/3.1.1/gallery/lines_bars_and_markers/barchart.html#sphx-glr-gallery-lines-bars-and-markers-barchart-py\n",
    "        \"\"\"\n",
    "        for rect in rects:\n",
    "            height = rect.get_height()\n",
    "            ax.annotate('{}'.format(height),\n",
    "                        xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                        xytext=(0, 3),  # 3 points vertical offset\n",
    "                        textcoords=\"offset points\",\n",
    "                        ha='center', va='bottom')\n",
    "            \n",
    "    ax.set_ylabel('Occurance')\n",
    "    ax.set_title('Symbols distribution in position')\n",
    "#     ax.set_xticks(np.arange(labels))\n",
    "    ax.set_xticklabels(labels)\n",
    "#     fig.tight_layout()\n",
    "\n",
    "\n",
    "def plot_unigram_statistics(filenames, output_dir=False, memory_comp=False):\n",
    "    \"\"\"\n",
    "    FOR FIGURE 3a\n",
    "    \n",
    "    Unigram statistics of symbol usage broken down by turn and by position within the utterance\n",
    "    for prosocial agents communicating via the linguistic channel.\n",
    "    \"\"\"\n",
    "    memory_comp = ['', 'memory_comp'][memory_comp]\n",
    "\n",
    "    for sociality, channel in gen_iter(exclude_channels=['proposal', 'none']):\n",
    "        key = sociality + '_' + channel\n",
    "        filename = filenames.get(key, None)\n",
    "        if filename:\n",
    "            print(filename)\n",
    "            extracted = extract_utterance(filename)\n",
    "            distribution, messages_count = get_distributions(extracted)\n",
    "            distribution = np.array(distribution)\n",
    "            for t in range(3):\n",
    "                temp_key = key + memory_comp + str(t)\n",
    "                plot_utterance(distribution / messages_count[t], t, output_dir=output_dir, key=temp_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_unigram_statistics(filenames, output_dir)\n",
    "plot_unigram_statistics(filenames_memory_comp, output_dir, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_bigram_statistics(filename, output_dir=False, key=False):\n",
    "    \"\"\"\n",
    "    FOR FIGURE 3b\n",
    "    \n",
    "    Bigram counts for prosocial agents communicating via the linguistic channel, sorted by frequency.\n",
    "    \"\"\"\n",
    "    \n",
    "    extracted = extract_utterance(filename)\n",
    "    extracted_a = extracted[::2]\n",
    "    extracted_b = extracted[1::2]\n",
    "    extracted_a = np.array([list(map(str, msg)) for sublist in extracted_a for msg in sublist])\n",
    "    extracted_b = np.array([list(map(str, msg)) for sublist in extracted_b for msg in sublist])\n",
    "\n",
    "    bigrams_a = []\n",
    "    bigrams_b = []\n",
    "    for i in range(extracted_a.shape[1] - 1):\n",
    "        new_bigrams_a = list(np.core.defchararray.add(extracted_a[:, i], extracted_a[:, i + 1]))\n",
    "        new_bigrams_b = list(np.core.defchararray.add(extracted_b[:, i], extracted_b[:, i + 1]))\n",
    "        bigrams_a += new_bigrams_a\n",
    "        bigrams_b += new_bigrams_b\n",
    "\n",
    "    for i in range(2):\n",
    "        \n",
    "        plt.clf()\n",
    "        plt.ylim([0, 0.3])\n",
    "        plt.xlim([0, 100])\n",
    "        bigrams = [bigrams_a, bigrams_b][i]\n",
    "        \n",
    "        unique, counts = np.unique(bigrams, return_counts=True)\n",
    "        counts, unique = order_lists(counts, unique)\n",
    "        x = np.arange(len(counts))\n",
    "        # normalization\n",
    "        counts = counts / sum(counts)\n",
    "        plt.bar(x, counts)\n",
    "        if output_dir and key:\n",
    "            output = '{}bigrams{}_{}.png'.format(output_dir, key, i)\n",
    "            plt.savefig(output)\n",
    "\n",
    "    \n",
    "def plot_bigram_statistics_all(filenames, output_dir=False, memory_comp=False):\n",
    "    \"\"\"\n",
    "    FOR FIGURE 3a\n",
    "    \n",
    "    Unigram statistics of symbol usage broken down by turn and by position within the utterance\n",
    "    for prosocial agents communicating via the linguistic channel.\n",
    "    \"\"\"\n",
    "    \n",
    "    memory_comp = ['', 'memory_comp'][memory_comp]\n",
    "    \n",
    "    for sociality, channel in gen_iter(exclude_channels=['proposal', 'none']):\n",
    "        key = sociality + '_' + channel\n",
    "        filename = filenames.get(key, None)\n",
    "        if filename:\n",
    "            plot_bigram_statistics(filename, output_dir=output_dir, key=key + memory_comp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bigram_statistics_all(filenames, output_dir=output_dir)\n",
    "plot_bigram_statistics_all(filenames_memory_comp, output_dir=output_dir, memory_comp=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "How many messages did agents send?\n",
    "\"\"\"\n",
    "\n",
    "filename = filenames['prosoc_linguistic']\n",
    "extracted = extract_utterance(filename)\n",
    "distribution, messages_count = get_distributions(extracted, n=10)\n",
    "agent1_count = sum(messages_count[::2])\n",
    "agent2_count = sum(messages_count[1::2])\n",
    "print('agent 1 count', agent1_count)\n",
    "print('agent 2 count', agent2_count)\n",
    "print('ratio', agent1_count/agent2_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Some additional calculations.\n",
    "\"\"\"\n",
    "\n",
    "filename = filenames['prosoc_both']\n",
    "extracted = extract_utterance(filename)\n",
    "distribution, messages_count = get_distributions(extracted, n=10)\n",
    "agent1_count = sum(messages_count[::2])\n",
    "agent2_count = sum(messages_count[1::2])\n",
    "print('agent 1 count', agent1_count)\n",
    "print('agent 2 count', agent2_count)\n",
    "print('ratio', agent1_count/agent2_count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
